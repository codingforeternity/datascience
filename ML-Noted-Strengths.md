ML is probably way too susceptible to overfitting to be used in many applications.  However, there are a lot of lessons that we can learn from it and the study of it.

  1. Representations - It may be exceptional at representing data (e.g. factorizing, dimension reduction) in efficient ways, such as with word embeddings (e.g. word2vec).
  2. Overfitting - Since ML is so susceptible to overfitting, and since the output of ML models aren't very understandable, there is a lot of focus placed on avoidance of overfitting, much more than there is with classic, statistical techniques.  For example, the realization that randomization of weights/inputs/activations is equivalent to weight penalties (see Hinton's lecture notes, 9b?) is a key result.
  3. Practical over theoretical - There really is no theory in ML, so practicality wins the day.  This does result, however, in a lot of trial-and-error (e.g. regarding different network architectures).  But again, techniques have been used well (e.g. cross-validation).
  4. Optimization efficiency - Different cost functions are used more often than in classic, statistical techniques.  Also, GPUs.