ML is probably way too susceptible to overfitting to be used in many applications.  However, there are a lot of lessons that we can learn from it and the study of it.

  1. **Representations** - It may be exceptional at representing data (e.g. factorizing, dimension reduction) in efficient ways, such as with word embeddings (e.g. word2vec).
  2. **Overfitting** - Since ML is so susceptible to overfitting, and since the output of ML models aren't very understandable, there is a lot of focus placed on avoidance of overfitting, much more than there is with classic, statistical techniques.  For example, the realization that randomization of weights/inputs/activations is equivalent to weight penalties (see Hinton's lecture notes, 9b?) is a key result.
    * **Sampling over Global Fitting** (aka Bayesian over Frequentist) - Sampling (in the form of: mini-batches, training/validation/test datasets, Mixture of Experts, boosting, bagging, corruption (e.g. SGNS, dropout, aka neuron sampling, denoising autoencoders) seems to have fantastic advantages wrt overfitting.
  3. **Practical over theoretical** - There really is no theory in ML, so practicality wins the day.  This does result, however, in a lot of trial-and-error (e.g. regarding different network architectures).  But again, techniques have been designed that can be applicable elsewhere (e.g. cross-validation).  *We aren't 'data scientists,' we're 'data engineers.'*
  4. **Optimization efficiency** - Different cost functions are used more often than in classic, statistical techniques.  Also, GPUs.
  5. **Noise is good** - SGD.  "Noisy [Hopfield] networks find better energy minima."  Dropout.  Normalization via noise.  Monte Carlo Markov Chains (for sampling representative distributions of random points).