Good lecture on word2vec and GloVe (10/19/16)
* Slides from Mar 31 lecture http://cs224d.stanford.edu/syllabus.html (which are also in email)

[Question answering on the Facebook bAbi dataset using recurrent neural networks and 175 lines of Python + Keras](http://smerity.com/articles/2015/keras_qa.html) (7/7/16)

[Google Has Open Sourced SyntaxNet, Its AI for Understanding Language](http://www.wired.com/2016/05/google-open-sourced-syntaxnet-ai-natural-language/) (5/13/16)
* "According to Google, Parsey McParseface is about 94 percent accurate in identifying how a word relates the rest of a sentence, a rate the company believes is close to the performance of a human (96 to 97 percent)."
* "people use language on the web in so many different ways. When Google trains its neural nets with this kind of dataset, the accuracy rate drops to about 90 percent"

NLP "features" shouldn't be functions of text passages, but rather correlations to sets of terms (just like QELA).  The relationships/correlations between the pairwise text-QELA terms are the features.

[State of the art models for anaphora/ coreference resolution?](https://www.reddit.com/r/MachineLearning/comments/4gptkp/state_of_the_art_models_for_anaphora_coreference/) (5/2/16)
* Here's the paper: [Learning Anaphoricity and Antecedent Ranking Features for Coreference Resolution](http://people.seas.harvard.edu/~srush/acl15.pdf)

[Sentiment Analysis in Python with TextBlob](https://github.com/shekhargulati/52-technologies-in-2016/blob/master/11-textblob/README.md) (3/19/16)

[Fuzzy string matching using cosine similarity](http://blog.nishtahir.com/2015/09/19/fuzzy-string-matching-using-cosine-similarity/)

Naive Bayes
* https://en.wikipedia.org/wiki/Naive_Bayes_classifier
* http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=MachineLearning